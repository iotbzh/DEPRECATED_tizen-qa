// ------------------ Document metadata ------------------------
// vim: set syntax=asciidoc spell: spelllang=en

include::common/header.asciidoc[]

:componentName: Component Name
:author: John Doe
:email: john@doe.com
:revdate: YYYY-MM-DD
:revnumber: 0.1
:title: {componentName} Test Plan
:status: Draft

// -------------------------------------------------------------

= Tizen PC - {title} 

Status: {status}

[red]*Intel Confidential*

== Revision History 

// Instructions:
// Provide a complete list of revision records for the test plan.
//
// Revision levels:
// 0.1: Is the very first draft containing the main features to be tested
// 0.2: ?
// 0.3: Enough content to scope effort: test scope, test strategy and approach are ready
// 0.4: Ready for QA internal review
// 0.5: Feedback from QA internal review incorporated; direction confirmed; test execution can be started
// 0.6: Ready for stakeholder public review. All relevant specifications are addressed and content complete
// 0.7: Feedbacks from stakeholder review incorporated; resent for ratification
// 0.8: Ratified and change control started

.Revisions
[width="80%",cols="1,1,2,4",options="header"]
|==============================================================================================
|Version 			| Date 			| Author											| Reason for changes
//---------------------------------------------------------------------------------------------
|0.1					| 2013-02-13	| {nobody}				               | Initial revision
|==============================================================================================

include::common/browser_warn.asciidoc[]

== Review History 

.Reviews
[width="80%",cols="2,4,1,1",options="header"]
|==============================================================================================
|Review Level 				| Reviewer 							| Start Date 	| Approve Date
//---------------------------------------------------------------------------------------------
|QA Internal review		| 										|           	|           
|Architect review			| 										|           	|           
|Engineering review 		| 										|           	|           
|Project Manager review | 										|           	|           
|==============================================================================================

== Scope of this Document 

This test plan defines test scope, test strategy and test execution for {componentName} within Tizen PC.

== Glossary 

// Instructions:
// Describe all important acronyms or terms here
// Example:
// * BBC: Brake Before Clutch (driver training)
// * CNN: Chaotic Neural Network
// ...

== Component Summary 

// Instructions:
// Expect a short paragraph to describe the component and add link for further information about this component.
// Also put major hyperlinks to projects responsible for the component

== Features to be Tested 

// Instructions
// List all of the features of this component.
// Assign a test priority to each feature and provide reasons. 
// This requires some risk analysis. Here Priority considerations are user impact and likelihood of defect.
// 
// Priority table:
//
//             \           Likelyhood
//              \  
//   User Impact \| High | Medium | Low        |
//  -------------------------------------------|
// | High         | P1   | P2     | P2         |
// | Medium       | P2   | P3     | P3         |
// | Low          | P2   | P3     | Not tested |
//  ------------------------------------------- 
//
// Risk analysis examples:
// * P2: Likelihood of defect in this library is low as it is from upstream and stable enough after many releases. Current version 3.2 is also utilized in Fedora. But it is a key package of all user experience(UX) applications. So user impact is High if it has a defect.
// * P1: Likelihood of defect in this library is high as it is a newly developed component from scratch. Moreover, it is a key package of all UX applications. So user impact is also High if it has a defect.

The following table describes features to be tested based on risk analysis:

.Tested features
[width="100%",cols="2,4,1,8",options="header"]
|===================================================================================================================
|Feature / Requirement Id          | Description                  | Priority | Comment
//------------------------------------------------------------------------------------------------------------------
// Fill here
|===================================================================================================================

== Features not to be Tested 

// Instructions:
// Identify any significant software features or other items that will not be tested.
// If necessary, explain why these items are not to be addressed in this test plan.
// Identify WHY the item is not to be tested. This could include such reasons as:
// * Not to be included in this release of the Software.
// * Low risk: has been used before and is considered stable.
// * Will be released but not tested or documented as a functional part of the release of the software.
// * Third-party features or components that will not be tested by our team.
// * Software features will be tested by other teams.
// * Documentation or legal requirements.

The following table describes features *not* to be tested:

.Non tested features
[width="100%",cols="2,4,9",options="header"]
|===================================================================================================================
|Feature / Requirement Id          | Description                  | Comment
//------------------------------------------------------------------------------------------------------------------
// Fill here
|===================================================================================================================

== Test Strategy and Approach 

// Instructions:
// Describe how the component will be tested.
// Specify refinements to the approach described in the project/product test plan.
// Include specific test techniques (such as test methodology, test framework, automation, test type, and test level) to be used.

=== Test Levels

// Instructions:
// Describe the test level (unit test, API level test, integration test, system test, and so on) for different sub-components.
// Test level will reflect the test priority of features.
// Test the high priority items extensively, medium priority items broadly, and the low-priority cursory.
//
// In general, unit test and integration testing are not covered in QA's test plan.
// If certain areas or aspects of the system imply high risks for the product, more thorough testing is obviously a solution.
// Focus testing effort on portions of the software where risk of potential failure is greatest or where potential failure would be most catastrophic.

==== Acceptance Test

// Instructions:
// Acceptance test set is fully automated. It can be run under Common Automated Test System (CATS) or Testkit to check distribution quickly after it's generated by release engineer.
// Acceptance test usually covers image installation, boot up, basic kernel and driver, core services, Power and Performance (PnP) and stability check points, which helps the release engineer to make a quick decision on if the current image is qualified for further integration and QA activities.

==== Sanity Test

// Instructions:
// Sanity test is a very brief run-through of the entire distribution, to ensure the basic health of the distribution and to report major regressions at the earliest time. All the checkpoints in a doc/TestPlan_template.asciidocsanity test reflect the most important functionality, stability and PnP of the distribution.

==== Feature Test

// Instructions:
// Feature test is designed to test product requirements or features. It includes both functional and non-functional (such as performance and stress) tests, as well as negative tests. 
// Feature Test covers both Feature Verification Test (FVT) and Extended Feature Test (EFT).

==== Feature Verification Test

// Instructions:
// FVT, the most key test for a feature, is designed to verify product features (in JIRA).
// FVT is conducted on a weekly basis when a feature is implemented and integrated to distribution.
// FVT should be reviewed by developers.
// Feature status update:
//    * A feature is marked Verified/Closed if its FVT passes.
//    * A feature is reopened if its FVT fails. In this case, related major bugs should be linked to the feature.

==== Extended Feature Test

// Instructions:
// EFT is designed to verify delivery of features forming full functionality of an entire component. 
// After the component is fully integrated, all componen-related test cases will be executed for selected release.
// In addition, all the bugs against the component and its features will be reported. 
// The EFT set will be run again in the upcoming milestone or when significant changes are applied to the component and its features.

==== DataFlow Test

// Instructions:
// Dataflow is the flow of data to, from and within a device. 
// Inputs are derived from system use cases. Each core dataflow test case presents the basic instance of the use case in core stack.
// The result of core dataflow test could report the maturity level of Core OS software and hardware integration on a specific device.

==== System Test (E2E Test)

// Instructions:
// System Test is targeting to evaluate delivered functionalities from a system perspective. 
// It tests how the entire system is working and interacting with consumers (end users) instead of user interface (UI) or application.
// Its test cases cover the most critical interaction and negative scenarios that consumers may encounter in their daily usage.
// Therefore, a system test is usually designed to cover product use cases.

==== PnP Test

// Instructions:
// PnP test set includes all system power and performance test cases, to get memory footprint, CPU consumption, response data, FPS, and smoothness with specific workload.

==== Stability Test

// Instructions:
// Stability test is designed to determine the robustness of software by testing beyond the limits of normal operation.
// Stability test commonly put greater emphasis on robustness, availability, and error handling under a heavy load, than on what would be considered correct behaviors under normal circumstances.
// Stability test focuses on checking product usages under low-resource, overloaded, recovery, repetitive, high intensity running, long-lasting and iterative operation conditions. This ensures that the operations work as expected without race condition, obvious hang or crash. The maximum amount of data and the failure occurrence rate are collected during the testing.

==== Usability Test

// Instructions:
// Usability test set defines test cases from an end user perspective, to check the effectiveness, efficiency and user satisfaction of a product.

==== Certification Test

// Instructions:
// 

==== Integration Test

// Instructions:
//

==== Conformance Test

// Instructions:
//

==== Interopeability Test

// Instructions:
//

=== Test Types

// Instructions:
// Describe how you will use different test types for the component, considering adopting the following test types in test design:
// * Feature Functional Test (positive, negative, internationalization and localization, and so on)
// * GUI Test if the component has GUI, validate if the implementation conforms to UI design.
// * User Experience Test if the component has GUI. Validate if the UI satisfy end users. Refer to related UX checklist defined for the project if any.doc/TestPlan_template.asciidoc
// * Stress Test for individual component to guarantee the reliability of component level.
// * Performance Test

&nbsp;

=== Test Methodology

// Instructions:
// If you adopt any specific test methodology to the component or sub-component testing, please give the summary and describe the method in details.
// Following are typical test methods:
//   * White-box Test Methods: Statement Coverage, Branch Coverage, Condition Coverage, Multiple Condition Coverage, Path Coverage, and so on
//   * Black-box Test Methods: Equivalence Partitioning, Boundary Value Analysis, Decision Table Testing, State Transition Testing, Use Case Testing, and so on
//   * Experience Based Methods: Error Guessing (weak point testing), Exploratory Testing, and so on

&nbsp;

=== Flexibility

// Instructions:
// Adjust your strategy if it produces an amount of necessary test effort or a time schedule that does not fit testing goals or project constraints

&nbsp;

=== Test Automation 

// Instructions:
// Describe test automation approach. The following contents are suggested
//   * Which part of tests will be automated
//   * Automation goal and automation percentage
//   * Follow the overall test automation strategy in project overall test plan
//   * The whole automated test suite low-level design which is used for guiding implementation. Create a dedicated document for it if required.
//   * Test framework and test automation tools to be used
//   * Test automation environment, including hardware, peripherals, software configuration, and so on
//   * Programming language

&nbsp;

== Test Design 

// Instructions:
// Adopt the test approach above to the component, define test points or design detailed test cases.
// Follow the common test design method if the project has defined it.
// Specify the test case organization and naming convention for this component.
// Specify the necessary test data used by test cases
// Specify the possible test set of the test cases. The test set can be determined by test case priority, test cycle, test purpose and so on. We have typical test sets as below:
// * Feature Test (FT)
// * Extended Feature Test (EFT)
// * Feature Verification Test (FVT): test cases used to verify feature integration.
// * Regression Test (RT) - test cases used for regression test.
//
// Typical test design should include the following aspects:
// * ID of the requirement to be tested
// * [Optional] Features extended by this requirement to be tested
// * Multiple test points derived from the requirement/feature to be tested
// * A test case or a set of test cases (following the naming convention) for the test point
// * Test case priority
// * Test set which test cases will serve

include::common/tc_design.asciidoc[]

*to be generated from qadb*

// Push titles down one level.
:leveloffset: 2
include::sample_testcases.asciidoc[]

// Return to normal title levels.
:leveloffset: 0

== Test Environment 

This section identifies everything required for the testing except the software itself.

=== Hardwares 

// Instructions:
// Target test platforms (URL of hardware wiki page if any), networking environment, peripherals, test and measurement equipment, and so on
//
// Test Platform                                 | Networking      | Other Devices                                          | Priority
// Pinetrail Netbook: HP mini 210                | LAN, WIFI (WPA) | BT earphone, USB disk, USB key, power meter, and so on | P1
// Diamondville Netbook: Acer Airespone One D150 | LAN, WiFi (WPA) | BT earphone, USB disk, USB key, power meter, and so on | P2

include::common/hardwares.asciidoc[]

=== Network configuration

// Instruction:
// Describe specific network config for tests

include::common/network_config.asciidoc[]

=== Tools 

// Instructions:
// Describe the software and hardware tools required for testing or test development.
// Following areas are recommended to consider:
// * tools as facility of test design
// * test case management tool: custom Git repository, Enterprise Tester
// * test automation tool: CATS 3.0 / Testkit-Lite
// * tools for test environment setup: custom
// * test infrastructure, execution framework or harness: custom
// * tools for test configuration and log collection: CATS 3.0
// * tools for test result comparison or analysis: QA Reports

include::common/tools.asciidoc[]

== Test Execution 

// Instructions:
// Describe in details the steps for setting up hardware and software environment and steps for test execution, including manual testing and automated testing.
// Refer to common test execution process of the project if any.

include::common/test_exec.asciidoc[]

== Test Reports and Metrics 

// Instructions:
// Specify what test reports will be delivered about this component, their cadenc, audience, templates, and so on .
// Refer to the common definition about test reports in project overall test plan if any.
// <Optional> Specify any test result or quality metrics from this component. Provide URLs, documents or templates about form of the metric delivery.
//
// Examples:
// * Pre-integration Test Report will be delivered to Developers, PM and stakeholders 
// * Weekly/ Milestone Test Report will be send out after Web Application integrated to vertical product 
// * Deliver Quality Metrics/Indicators to Web App Initiative PDT

include::common/test_report.asciidoc[]

== Bug Tracking

// Instructions:
// Describe how bugs are reported, triaged etc.

include::common/bug_tracking.asciidoc[]

== Risks and Limitations 

// Instructions:
// This section lists any risks and limitations that may affect the design, development or implementation of testing, including:
// * Unavailable documentations: requirement, high level design, UI design, and so on
// * Untestable requirements or features
// * Unclear POR (Project Objectives Review?)
// * Unclear QA ownership
// * Lack of test devices, tools or environment
// * Technique limitations for test development
// * Resource limitation for deep testing and so on

== Contacts

// Instructions:
// Describe people involved in the component

.Contacts
[width="80%",cols="1,2",options="header"]
|==============================================================================================
|Role          		             | Contact               
//---------------------------------------------------------------------------------------------
|Architect	                      |
|Designer	                      |
|Engineering Manager              |
|Developers	                      |
|PM			                      |
|Marketing					          |
|Test planning and test case Lead | {yann}
|QA			                      |
|QA Conformance			          |
|==============================================================================================

== References 

// Instructions:
// Place the references here, for example:
// * Requirement Documentations 
// * Component architecture or design docs
// * Project Overall or Master Test Plan
// * Test Method related references
// * Test Process related references
// * Urls or Docs about test environment and tools

// http://tz.otcshare.org/xxx/yyy[PRD (Product Requirements Document)]
// http://tz.otcshare.org/xxx/yyy[Design document]

Requirements:
--
* None [red]#FIXME#
--

include::common/references.asciidoc[]
